## Exam Preparation

### Week 1: Introduction to Natural Language Processing (NLP)

#### Key NLP Tasks
- Text categorization
- Document summarization
- Machine translation (50% of web pages are in languages other than English)
- Question answering

#### Pre-Deep Learning Approaches
- Support Vector Machines (SVM)
- Decision Trees
- Generative Bayesian Approaches
- Maximum Entropy Approaches

#### Neural Networks in NLP
**Advantages:**
- No feature engineering required

**Disadvantages:**
- Large amounts of training data needed
- Difficulty in tracing errors

#### Word-Based Models
**Feed-Forward Language Model (FFNN)**
- Language modeling
- Word representation (embedding)

**Disadvantages:**
- Restricted to fixed sentence length

#### Sequence Classification
**Convolutional Neural Networks (CNN)**
- Language modeling leading to summarization and translation
- Sentence classification
- Summarizes vectors in the top layer by pooling

#### Sequence Labeling
**Recurrent Neural Networks (RNN) Language Models**
- Summarization
- Sequence labeling (Part-of-Speech tagging, Named Entity Recognition)
- Machine translation

#### Encoder-Decoder Architecture
- Can use CNN or RNN for encoding
- Uses RNN for decoding
- Applications: Summarization, image captioning

#### Sequence-to-Sequence Modeling
**Neural Machine Translation**
- Uses RNN and Transformer architectures
- Transformers can perform Q&A and machine translation

#### Large Language Models
- Fine-tuning techniques

### Week 2

